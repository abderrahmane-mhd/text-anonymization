{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abderrahmane-mhd/text-anonymization/blob/main/TextAnonymization.ipynb)\n"
      ],
      "metadata": {
        "id": "nCeVwJqrHLRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Anonymization\n",
        "\n",
        "\n",
        "The upcoming notebook will showcase the implementation of an anonymization pipeline, utilizing Presidio - a comprehensive anonymization library. During the development of the pipeline, we will compare and evaluate various methods of text anonymization, in order to determine the most effective approach for the given task.\n",
        "\n",
        "* The first method we will explore is the use of Named Entity Recognition (NER) models, implemented using both Spacy and Transformers frameworks. NER models are capable of identifying and labeling specific types of named entities in text, such as names, locations, and organizations, which can then be replaced with anonymized placeholders.\n",
        "\n",
        "* Next, we will also explore the use of a Part-Of-Speech (POS) tagging model, implemented using Transformers, which labels each word in a given text with a corresponding part of speech. This can help identify certain words that should be anonymized, such as verbs, adjectives, or adverbs.\n",
        "\n",
        "* Finally, we will integrate both Transformers and Spacy models to create a hybrid approach to text anonymization, which may yield better results than using either method independently. By comparing the performance and effectiveness of each approach, we aim to provide valuable insights into the most effective techniques for anonymizing sensitive data.\n",
        "\n",
        "PS: In the following notebook, I've assessed the anonymization model's performance on two distinct text categories: narrative-style text and call recording transcripts, as these represent typical applications for NLP models."
      ],
      "metadata": {
        "id": "TvzX080RRN2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have started by generating a French Text with annotations using the following prompt in ChatGPT app"
      ],
      "metadata": {
        "id": "ob2dwpzURZf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_fr = \"\"\"Il était une fois, dans la ville de Paris, une jeune femme nommée Marie qui travaillait pour l'Organisation des Nations unies pour l'alimentation et l'agriculture (FAO). Elle avait pour mission de se rendre dans différents pays pour étudier les pratiques agricoles locales et promouvoir des méthodes durables pour nourrir la population mondiale croissante. Marie était passionnée par son travail et avait déjà visité de nombreux endroits, tels que le Brésil, le Kenya et l'Inde.\n",
        "\n",
        "Un jour, alors qu'elle se trouvait en mission en Égypte, Marie rencontra un homme charmant du nom d'Ahmed. Il travaillait pour une organisation locale appelée l'Association égyptienne pour le développement rural (AEDR), qui avait pour objectif d'améliorer les conditions de vie des communautés rurales en Égypte. Ahmed était également très intéressé par l'agriculture durable et lui et Marie se lièrent d'amitié rapidement.\n",
        "\n",
        "Marie et Ahmed ont continué à travailler ensemble pour promouvoir des pratiques agricoles durables en Égypte et ont finalement créé une organisation conjointe appelée l'Initiative pour une agriculture durable (IAS). Ils ont réussi à obtenir un financement de l'Union européenne pour leur projet et ont pu étendre leur travail à d'autres pays du Moyen-Orient.\n",
        "\n",
        "Au fil des années, l'IAS est devenue une organisation de premier plan dans le domaine de l'agriculture durable et a reçu de nombreux prix pour son travail. Marie et Ahmed ont continué à diriger l'organisation ensemble et ont fondé une famille heureuse en Égypte. Leur travail a permis d'améliorer la vie de nombreuses personnes dans le monde entier et leur héritage continue de vivre à travers l'IAS, qui continue à promouvoir des pratiques agricoles durables et à lutter contre la faim dans le monde.\"\"\""
      ],
      "metadata": {
        "id": "CHOZ5qIVffk3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = {\"Paris\": \"LOC\", \"Marie\": \"PER\", \"Organisation des Nations unies pour l'alimentation et l'agriculture (FAO)\": \"ORG\", \"Brésil\": \"LOC\", \"Kenya\": \"LOC\", \"Inde\": \"LOC\", \"Égypte\": \"LOC\", \"Ahmed\": \"PER\", \"Association égyptienne pour le développement rural (AEDR)\": \"ORG\", \"Initiative pour une agriculture durable (IAS)\": \"ORG\", \"Union européenne\": \"ORG\", \"Moyen-Orient\": \"LOC\"}\n"
      ],
      "metadata": {
        "id": "zmTkjSvCfs9I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text_fr = \"\"\"Bonjour, Jean Dupont ici. Je suis intéressé par l'achat d'un nouvel appareil électronique. Je me demandais si vous pourriez m'aider à trouver le meilleur modèle pour mes besoins.\n",
        "\n",
        "Bonjour Monsieur Dupont, je suis Sophie Martin de la société Techno Plus. Bien sûr, je serais heureuse de vous aider. Quels sont les spécifications techniques que vous recherchez ?\n",
        "\n",
        "Eh bien, je cherche un modèle avec une grande capacité de stockage, une haute résolution d'écran et un processeur rapide.\n",
        "\n",
        "D'accord, nous avons plusieurs modèles qui pourraient correspondre à ces spécifications. L'un d'eux est notre modèle haut de gamme, qui dispose d'un écran OLED et d'un processeur quad-core. Cependant, il est plus cher que nos modèles standard.\n",
        "\n",
        "Je vois. Et quels sont les autres modèles disponibles ?\n",
        "\n",
        "Nous avons également notre modèle standard, qui a une capacité de stockage de base mais une résolution d'écran similaire. Il est moins cher que notre modèle haut de gamme. Nous avons également un modèle intermédiaire qui est un peu plus cher que le modèle standard mais offre une meilleure capacité de stockage.\n",
        "\n",
        "Hmm, c'est difficile de choisir. Pourriez-vous me donner le prix de chacun de ces modèles ?\n",
        "\n",
        "Bien sûr, le modèle standard est à 599 euros, le modèle intermédiaire est à 799 euros et le modèle haut de gamme est à 999 euros.\n",
        "\n",
        "Très bien, merci pour ces informations. Et pouvez-vous me dire où est située votre entreprise ?\n",
        "\n",
        "Nous sommes basés à Tokyo, mais nous avons des centres de distribution dans le monde entier.\n",
        "\n",
        "D'accord, merci. Et pour la commande, comment ça se passe ?\n",
        "\n",
        "Vous pouvez commander en ligne sur notre site Web, ou vous pouvez nous appeler directement pour passer une commande. Si vous voulez commander par téléphone, voici notre numéro : +33 1 23 45 67 89.\n",
        "\n",
        "Très bien, je prends note de ça. Et pour la livraison, je peux la faire livrer à une adresse spécifique ?\n",
        "\n",
        "Oui, bien sûr. Vous pouvez nous donner l'adresse de livraison et nous nous occuperons du reste.\n",
        "\n",
        "Parfait, merci beaucoup pour votre aide, Sophie.\n",
        "\n",
        "De rien, c'était un plaisir de vous aider, Monsieur Dupont. Si vous avez d'autres questions, n'hésitez pas à nous contacter à nouveau.\"\"\""
      ],
      "metadata": {
        "id": "cl1_ySKsYJPI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Installation\n"
      ],
      "metadata": {
        "id": "7Z6_2HoegIPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio-analyzer\n",
        "!pip install presidio-anonymizer\n",
        "!python -m spacy download fr_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZOaI-QVgLbd",
        "outputId": "871a0e6a-07e2-475f-8143-59ff184ae038"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting presidio-analyzer\n",
            "  Downloading presidio_analyzer-2.2.33-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (3.6.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (2023.6.3)\n",
            "Collecting tldextract (from presidio-analyzer)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (6.0.1)\n",
            "Collecting phonenumbers>=8.12 (from presidio-analyzer)\n",
            "  Downloading phonenumbers-8.13.19-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.4.4->presidio-analyzer) (3.3.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer) (3.4)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer) (3.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.4.4->presidio-analyzer) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.4.4->presidio-analyzer) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.4.4->presidio-analyzer) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio-analyzer) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio-analyzer) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.4.4->presidio-analyzer) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract->presidio-analyzer) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio-analyzer) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.4.4->presidio-analyzer) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.4.4->presidio-analyzer) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.4.4->presidio-analyzer) (2.1.3)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio-analyzer\n",
            "Successfully installed phonenumbers-8.13.19 presidio-analyzer-2.2.33 requests-file-1.5.1 tldextract-3.4.4\n",
            "Collecting presidio-anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.33-py3-none-any.whl (28 kB)\n",
            "Collecting pycryptodome>=3.10.1 (from presidio-anonymizer)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome, presidio-anonymizer\n",
            "Successfully installed presidio-anonymizer-2.2.33 pycryptodome-3.18.0\n",
            "2023-09-02 17:45:53.445397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-02 17:45:54.744389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting fr-core-news-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.6.0/fr_core_news_lg-3.6.0-py3-none-any.whl (571.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-lg\n",
            "Successfully installed fr-core-news-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2V1oq-zge11",
        "outputId": "fac258f8-44ca-46ee-d557-ab50cee30ee1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE_mvGOqk2b2",
        "outputId": "a3792f69-d868-4558-e479-4c0246bda87e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. NER SpaCy Model\n",
        "\n",
        "We will start by trying NER model proposed by SpaCy"
      ],
      "metadata": {
        "id": "obOYQYOLiMiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_lg\")"
      ],
      "metadata": {
        "id": "weU3QnrUiP_Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult, Pattern, PatternRecognizer\n",
        "\n",
        "from presidio_analyzer.nlp_engine import NlpArtifacts,NlpEngineProvider\n"
      ],
      "metadata": {
        "id": "6rYe0edLK3TB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = {\"nlp_engine_name\":\"spacy\", \"models\":[{\"lang_code\":\"fr\", \"model_name\":\"fr_core_news_lg\"}]}\n",
        "\n",
        "\n",
        "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
        "\n",
        "nlp_engine = provider.create_engine()\n",
        "\n",
        "\n",
        "analyzer = AnalyzerEngine(\n",
        "    nlp_engine=nlp_engine,\n",
        "    supported_languages = ['fr']\n",
        ")"
      ],
      "metadata": {
        "id": "tGLP7z2PK5B1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = analyzer.analyze(text=text_fr, language='fr')\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0647MAbLSnA",
        "outputId": "45f8ca38-91e4-41ae-a0cc-e102e119e940"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[type: LOCATION, start: 36, end: 41, score: 0.85, type: PERSON, start: 66, end: 71, score: 0.85, type: PERSON, start: 357, end: 362, score: 0.85, type: LOCATION, start: 451, end: 457, score: 0.85, type: LOCATION, start: 462, end: 467, score: 0.85, type: LOCATION, start: 473, end: 477, score: 0.85, type: LOCATION, start: 529, end: 535, score: 0.85, type: PERSON, start: 537, end: 542, score: 0.85, type: PERSON, start: 580, end: 585, score: 0.85, type: LOCATION, start: 785, end: 791, score: 0.85, type: PERSON, start: 793, end: 798, score: 0.85, type: PERSON, start: 866, end: 871, score: 0.85, type: PERSON, start: 905, end: 910, score: 0.85, type: PERSON, start: 914, end: 919, score: 0.85, type: LOCATION, start: 1007, end: 1013, score: 0.85, type: LOCATION, start: 1250, end: 1262, score: 0.85, type: PERSON, start: 1421, end: 1426, score: 0.85, type: PERSON, start: 1430, end: 1435, score: 0.85, type: LOCATION, start: 1520, end: 1526, score: 0.85]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "found_entities = [text_fr[obj.to_dict()['start']:obj.to_dict()['end']] for obj in result]\n",
        "\n",
        "unique_entities = set(found_entities)\n",
        "\n",
        "print(unique_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iEgepz0LcCK",
        "outputId": "1839ac73-78bd-47a8-85c8-39b89fc41626"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Inde', 'Marie', 'Paris', 'Égypte', 'Kenya', 'Ahmed', 'Moyen-Orient', 'Brésil'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(annotations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grcQiHuiSBqu",
        "outputId": "006e2686-18cb-46ba-e21d-2fda24612d3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Ahmed',\n",
              " 'Association égyptienne pour le développement rural (AEDR)',\n",
              " 'Brésil',\n",
              " 'Inde',\n",
              " 'Initiative pour une agriculture durable (IAS)',\n",
              " 'Kenya',\n",
              " 'Marie',\n",
              " 'Moyen-Orient',\n",
              " \"Organisation des Nations unies pour l'alimentation et l'agriculture (FAO)\",\n",
              " 'Paris',\n",
              " 'Union européenne',\n",
              " 'Égypte'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = len(set(unique_entities) & set(annotations)) / len(annotations)\n",
        "print(f\"Accuracy score: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4rUZqVdMV3H",
        "outputId": "68f092aa-0fd1-46a3-93ba-7d824a6af4c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER Transformers Model\n",
        "\n",
        "In this section we try out a Name Entity Recognition Model using a transformer model CamemBERT adapted for French Text"
      ],
      "metadata": {
        "id": "tQwYjmQwytGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "# Loading both tokenizer and NER model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
      ],
      "metadata": {
        "id": "DgnKOudFGP4k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline('ner', model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "transformer_res = nlp(text_fr)"
      ],
      "metadata": {
        "id": "CpASFtAeKB-4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detected_entities = [res['word'] for res in transformer_res]"
      ],
      "metadata": {
        "id": "paFg-PsUKPWX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(detected_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0eSIsEKnka",
        "outputId": "af4979d1-82e1-4532-edd7-a6463bd63fb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AEDR',\n",
              " 'Ahmed',\n",
              " 'Association égyptienne pour le développement rural',\n",
              " 'Brésil',\n",
              " 'FAO',\n",
              " 'IAS',\n",
              " 'Inde',\n",
              " 'Initiative pour une agriculture durable',\n",
              " 'Kenya',\n",
              " 'Marie',\n",
              " 'Moyen-Orient',\n",
              " \"Organisation des Nations unies pour l'alimentation et l'agriculture\",\n",
              " 'Paris',\n",
              " 'Union européenne',\n",
              " 'Égypte'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the NER transformer model was successful in detecting all the entities, it identified some organizations and abbreviations separately, such as \"*Initiative pour une agriculture durable*\" and \"*IAS*\". Despite this difference, it doesn't impact the overall anonymization process since the identified entities will still be anonymized."
      ],
      "metadata": {
        "id": "DHFhikSrSi-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixed Pipeline development (Transformers + SpaCy)"
      ],
      "metadata": {
        "id": "k-M3WIMPTL_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
        "DEFAULT_ANOYNM_ENTITIES = [\n",
        "    \"CREDIT_CARD\",\n",
        "    \"CRYPTO\",\n",
        "    \"DATE_TIME\",\n",
        "    \"EMAIL_ADDRESS\",\n",
        "    \"IBAN_CODE\",\n",
        "    \"IP_ADDRESS\",\n",
        "    \"NRP\",\n",
        "    \"LOCATION\",\n",
        "    \"PERSON\",\n",
        "    \"PHONE_NUMBER\",\n",
        "    \"MEDICAL_LICENSE\",\n",
        "    \"URL\",\n",
        "    \"ORGANIZATION\",\n",
        "    \"NUMBER\"\n",
        "]\n",
        "\n",
        "class TransformerRecognizer(EntityRecognizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id_or_path,\n",
        "        mapping_labels,\n",
        "        aggregation_strategy=\"simple\",\n",
        "        supported_language=\"fr\",\n",
        "        ignore_labels=[\"O\", \"MISC\"],\n",
        "    ):\n",
        "        # inits transformers pipeline for given mode or path\n",
        "        self.pipeline = pipeline(\n",
        "            \"token-classification\", model=model_id_or_path, aggregation_strategy=aggregation_strategy, ignore_labels=ignore_labels\n",
        "        )\n",
        "        # map labels to presidio labels\n",
        "        self.label2presidio = mapping_labels\n",
        "\n",
        "        # passes entities from model into parent class\n",
        "        super().__init__(supported_entities=list(self.label2presidio.values()), supported_language=supported_language)\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"No loading is required.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def analyze(\n",
        "        self, text: str, entities = None, nlp_artifacts: NlpArtifacts = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Extracts entities using Transformers pipeline\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        predicted_entities = self.pipeline(text)\n",
        "        if len(predicted_entities) > 0:\n",
        "            for e in predicted_entities:\n",
        "                if(e['entity_group'] not in self.label2presidio):\n",
        "                    continue\n",
        "                converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
        "                if converted_entity in entities or entities is None:\n",
        "                    results.append(\n",
        "                        RecognizerResult(\n",
        "                            entity_type=converted_entity, start=e[\"start\"], end=e[\"end\"], score=e[\"score\"]\n",
        "                        )\n",
        "                    )\n",
        "        return results"
      ],
      "metadata": {
        "id": "hgPuExQ7GN7B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping_labels = {\"PROPN\": \"PERSON\",\"XFAMIL\": \"PERSON\"}\n",
        "mapping_labels = {\"PER\":\"PERSON\",'LOC':'LOCATION','ORG':\"ORGANIZATION\",'PHONE_NUMBER':'PHONE_NUMBER'}\n",
        "configuration = {\"nlp_engine_name\":\"spacy\",\n",
        "                \"models\":[{\"lang_code\": 'fr', \"model_name\":\"fr_core_news_lg\"}]}\n",
        "\n",
        "\n",
        "to_keep = []\n",
        "lang = 'fr'"
      ],
      "metadata": {
        "id": "80SbWEtDHGBa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
        "nlp_engine = provider.create_engine()\n",
        "\n",
        "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
        "analyzer = AnalyzerEngine(\n",
        "    nlp_engine=nlp_engine,\n",
        "    supported_languages = \"fr\"\n",
        ")\n",
        "\n",
        "transformers_recognizer = TransformerRecognizer(\"Jean-Baptiste/camembert-ner\", mapping_labels)\n",
        "analyzer.registry.add_recognizer(transformers_recognizer)"
      ],
      "metadata": {
        "id": "Xe2qcwhaGbI7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Analyzer\n",
        "analyzer_results = analyzer.analyze(text=new_text_fr, entities = DEFAULT_ANOYNM_ENTITIES, allow_list = to_keep, language=lang)\n",
        "\n",
        "# Text Anonymizer\n",
        "engine = AnonymizerEngine()\n",
        "result = engine.anonymize(text=new_text_fr, analyzer_results=analyzer_results)\n",
        "\n",
        "# Restructuring anonymizer results\n",
        "\n",
        "anonymization_results =  {\"anonymized\": result.text,\"found\": [entity.to_dict() for entity in analyzer_results]}\n",
        "\n",
        "words = [{'word': new_text_fr[obj['start']:obj['end']], 'entity_type':obj['entity_type'], 'start':obj['start'], 'end':obj['end']} for obj in anonymization_results['found']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpJyGgN3OvCZ",
        "outputId": "5af5165b-aed3-4c6c-be58-f4fed7442946"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Entity CREDIT_CARD doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity CRYPTO doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity DATE_TIME doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity EMAIL_ADDRESS doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity IBAN_CODE doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity IP_ADDRESS doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity NRP doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity MEDICAL_LICENSE doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity URL doesn't have the corresponding recognizer in language : fr\n",
            "WARNING:presidio-analyzer:Entity NUMBER doesn't have the corresponding recognizer in language : fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gkqt7clQSvF",
        "outputId": "ec09da76-4702-478c-bb39-47d1ac479ff6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'word': ' Tokyo', 'entity_type': 'LOCATION', 'start': 1440, 'end': 1446},\n",
              " {'word': ' Sophie Martin', 'entity_type': 'PERSON', 'start': 212, 'end': 226},\n",
              " {'word': ' Techno Plus',\n",
              "  'entity_type': 'ORGANIZATION',\n",
              "  'start': 240,\n",
              "  'end': 252},\n",
              " {'word': ' Monsieur Dupont',\n",
              "  'entity_type': 'PERSON',\n",
              "  'start': 187,\n",
              "  'end': 203},\n",
              " {'word': ' Monsieur Dupont',\n",
              "  'entity_type': 'PERSON',\n",
              "  'start': 2070,\n",
              "  'end': 2086},\n",
              " {'word': ' Jean Dupont', 'entity_type': 'PERSON', 'start': 8, 'end': 20},\n",
              " {'word': ' Sophie', 'entity_type': 'PERSON', 'start': 2018, 'end': 2025}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We had to strip the results to remove the leading spaces and \\n\n",
        "word_results = [res['word'].strip() for res in words]"
      ],
      "metadata": {
        "id": "WVwg51TLI1Km"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(word_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_3rIBumJA_5",
        "outputId": "444e4d28-d06c-4b04-918d-396909618ef6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Jean Dupont',\n",
              " 'Monsieur Dupont',\n",
              " 'Sophie',\n",
              " 'Sophie Martin',\n",
              " 'Techno Plus',\n",
              " 'Tokyo'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anonymization_results['anonymized']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "iAkVkk0fYOoW",
        "outputId": "0f9d8933-1112-459e-9537-5100827afb8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Bonjour,<PERSON> ici. Je suis intéressé par l'achat d'un nouvel appareil électronique. Je me demandais si vous pourriez m'aider à trouver le meilleur modèle pour mes besoins.\\n\\nBonjour<PERSON>, je suis<PERSON> de la société<ORGANIZATION>. Bien sûr, je serais heureuse de vous aider. Quels sont les spécifications techniques que vous recherchez ?\\n\\nEh bien, je cherche un modèle avec une grande capacité de stockage, une haute résolution d'écran et un processeur rapide.\\n\\nD'accord, nous avons plusieurs modèles qui pourraient correspondre à ces spécifications. L'un d'eux est notre modèle haut de gamme, qui dispose d'un écran OLED et d'un processeur quad-core. Cependant, il est plus cher que nos modèles standard.\\n\\nJe vois. Et quels sont les autres modèles disponibles ?\\n\\nNous avons également notre modèle standard, qui a une capacité de stockage de base mais une résolution d'écran similaire. Il est moins cher que notre modèle haut de gamme. Nous avons également un modèle intermédiaire qui est un peu plus cher que le modèle standard mais offre une meilleure capacité de stockage.\\n\\nHmm, c'est difficile de choisir. Pourriez-vous me donner le prix de chacun de ces modèles ?\\n\\nBien sûr, le modèle standard est à 599 euros, le modèle intermédiaire est à 799 euros et le modèle haut de gamme est à 999 euros.\\n\\nTrès bien, merci pour ces informations. Et pouvez-vous me dire où est située votre entreprise ?\\n\\nNous sommes basés à<LOCATION>, mais nous avons des centres de distribution dans le monde entier.\\n\\nD'accord, merci. Et pour la commande, comment ça se passe ?\\n\\nVous pouvez commander en ligne sur notre site Web, ou vous pouvez nous appeler directement pour passer une commande. Si vous voulez commander par téléphone, voici notre numéro : +33 1 23 45 67 89.\\n\\nTrès bien, je prends note de ça. Et pour la livraison, je peux la faire livrer à une adresse spécifique ?\\n\\nOui, bien sûr. Vous pouvez nous donner l'adresse de livraison et nous nous occuperons du reste.\\n\\nParfait, merci beaucoup pour votre aide,<PERSON>.\\n\\nDe rien, c'était un plaisir de vous aider,<PERSON>. Si vous avez d'autres questions, n'hésitez pas à nous contacter à nouveau.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There may be situations where Named Entity Recognition (NER) models are unable to identify certain words that need to be anonymized. In such cases, Part-Of-Speech (POS) Tagging can be used as an alternative approach to ensure stricter anonymization. POS tagging involves labeling each word in a text with a corresponding part of speech, such as noun, verb, adjective, or adverb. This can help in identifying specific types of words that need to be anonymized, such as names, locations, or organizations. By combining the results of both NER and POS tagging, we can achieve a more comprehensive and accurate approach to anonymization, which is particularly important in cases where data privacy is a concern."
      ],
      "metadata": {
        "id": "zh8sbOb0OGPB"
      }
    }
  ]
}